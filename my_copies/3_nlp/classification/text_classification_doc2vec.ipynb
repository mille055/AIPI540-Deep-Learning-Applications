{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmsDKgbwxg2S"
      },
      "source": [
        "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lGsXGsKxg2T"
      },
      "source": [
        "# Text Classification using Doc2Vec embeddings\n",
        "In this notebook we will be doing text classification using document embeddings obtained using a pre-trained Doc2Vec model.  The Doc2Vec algorithm was introduced in 2014 by Le and Mikolov to overcome the issues associated with simple averaging of Word2Vec vectors to form a representation of a document as an average of the words in the document.  Doc2Vec creates a numerical embedding for a document by embedding all words in the document (as Word2Vec does) but then also creates an additional vector representing the entire document which contributes to the training predictions.\n",
        "\n",
        "Our goal will be to classify the articles in the AgNews dataset into their correct category: \"World\", \"Sports\", \"Business\", or \"Sci/Tec\".\n",
        "\n",
        "**Notes:**  \n",
        "- This does not need to be run on GPU, but will take 5-10 minutes on CPU  \n",
        "\n",
        "**References:**  \n",
        "- Read the original [Doc2Vec paper](https://arxiv.org/pdf/1405.4053v2.pdf) by Le & Mikolov  \n",
        "- Some portions of this code are from the [GENSIM docs Doc2Vec tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "701gknmwxg2U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "#!python -m spacy download en_core_web_md\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "import gensim\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fvN_3u-fxg2V",
        "outputId": "6e35c2c2-05bf-4618-ccdd-7134c13f2731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Class Index                                              Title  \\\n",
              "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
              "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
              "2            3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
              "3            3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
              "4            3  Oil prices soar to all-time record, posing new...   \n",
              "\n",
              "                                         Description  \\\n",
              "0  Reuters - Short-sellers, Wall Street's dwindli...   \n",
              "1  Reuters - Private investment firm Carlyle Grou...   \n",
              "2  Reuters - Soaring crude prices plus worries\\ab...   \n",
              "3  Reuters - Authorities have halted oil export\\f...   \n",
              "4  AFP - Tearaway world oil prices, toppling reco...   \n",
              "\n",
              "                                           full_text  \n",
              "0  Wall St. Bears Claw Back Into the Black (Reute...  \n",
              "1  Carlyle Looks Toward Commercial Aerospace (Reu...  \n",
              "2  Oil and Economy Cloud Stocks' Outlook (Reuters...  \n",
              "3  Iraq Halts Oil Exports from Main Southern Pipe...  \n",
              "4  Oil prices soar to all-time record, posing new...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25961c8b-589c-4292-982f-1aeeea7d088c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class Index</th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
              "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
              "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
              "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
              "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
              "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
              "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
              "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Oil prices soar to all-time record, posing new...</td>\n",
              "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
              "      <td>Oil prices soar to all-time record, posing new...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25961c8b-589c-4292-982f-1aeeea7d088c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-25961c8b-589c-4292-982f-1aeeea7d088c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-25961c8b-589c-4292-982f-1aeeea7d088c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Download the data\n",
        "if not os.path.exists('../data'):\n",
        "    os.mkdir('../data')\n",
        "if not os.path.exists('../data/agnews'):\n",
        "    url = 'https://storage.googleapis.com/aipi540-datasets/agnews.zip'\n",
        "    urllib.request.urlretrieve(url,filename='../data/agnews.zip')\n",
        "    zip_ref = zipfile.ZipFile('../data/agnews.zip', 'r')\n",
        "    zip_ref.extractall('../data/agnews')\n",
        "    zip_ref.close()\n",
        "\n",
        "train_df = pd.read_csv('../data/agnews/train.csv')\n",
        "test_df = pd.read_csv('../data/agnews/test.csv')\n",
        "\n",
        "# Combine title and description of article to use as input documents for model\n",
        "train_df['full_text'] = train_df.apply(lambda x: ' '.join([x['Title'],x['Description']]),axis=1)\n",
        "test_df['full_text'] = test_df.apply(lambda x: ' '.join([x['Title'],x['Description']]),axis=1)\n",
        "\n",
        "# Create dictionary to store mapping of labels\n",
        "ag_news_label = {1: \"World\",\n",
        "                 2: \"Sports\",\n",
        "                 3: \"Business\",\n",
        "                 4: \"Sci/Tec\"}\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0cMCSakyxg2V",
        "outputId": "cf2d5c96-895c-464f-8e6e-9e89a8659c6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
            "\n",
            "Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
            "\n",
            "Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
            "\n",
            "Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
            "\n",
            "Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# View a couple of the documents\n",
        "for i in range(5):\n",
        "    print(train_df.iloc[i]['full_text'])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj6fFIR1xg2W"
      },
      "source": [
        "## Pre-process text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "myUY3lT4xg2W"
      },
      "outputs": [],
      "source": [
        "# Function to generate gensim tokens from a text corpus\n",
        "import smart_open\n",
        "def read_corpus(fname, tokens_only=False):\n",
        "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            tokens = gensim.utils.simple_preprocess(line)\n",
        "            if tokens_only:\n",
        "                yield tokens\n",
        "            else:\n",
        "                # For training data, add tags\n",
        "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
        "\n",
        "# Function to generate gensim tokens from a list of text docs \n",
        "def read_corpus_from_list(lst, tokens_only=False):\n",
        "        for i, line in enumerate(lst):\n",
        "            tokens = gensim.utils.simple_preprocess(line)\n",
        "            if tokens_only:\n",
        "                yield tokens\n",
        "            else:\n",
        "                # For training data, add tags\n",
        "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_OS-WYmxxg2W"
      },
      "outputs": [],
      "source": [
        "# Read in the training corpus\n",
        "corpus = list(read_corpus_from_list(train_df['full_text'].tolist()))\n",
        "\n",
        "# Tokenize the training and test sets\n",
        "train_tokens = list(read_corpus_from_list(train_df['full_text'].tolist(),tokens_only=True))\n",
        "test_tokens = list(read_corpus_from_list(test_df['full_text'].tolist(),tokens_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_tokens[:10]"
      ],
      "metadata": {
        "id": "-fq4UbReygbp",
        "outputId": "bc6ef4d5-e51f-4f70-f2cc-776be89d88cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['fears',\n",
              "  'for',\n",
              "  'pension',\n",
              "  'after',\n",
              "  'talks',\n",
              "  'unions',\n",
              "  'representing',\n",
              "  'workers',\n",
              "  'at',\n",
              "  'turner',\n",
              "  'newall',\n",
              "  'say',\n",
              "  'they',\n",
              "  'are',\n",
              "  'disappointed',\n",
              "  'after',\n",
              "  'talks',\n",
              "  'with',\n",
              "  'stricken',\n",
              "  'parent',\n",
              "  'firm',\n",
              "  'federal',\n",
              "  'mogul'],\n",
              " ['the',\n",
              "  'race',\n",
              "  'is',\n",
              "  'on',\n",
              "  'second',\n",
              "  'private',\n",
              "  'team',\n",
              "  'sets',\n",
              "  'launch',\n",
              "  'date',\n",
              "  'for',\n",
              "  'human',\n",
              "  'spaceflight',\n",
              "  'space',\n",
              "  'com',\n",
              "  'space',\n",
              "  'com',\n",
              "  'toronto',\n",
              "  'canada',\n",
              "  'second',\n",
              "  'team',\n",
              "  'of',\n",
              "  'rocketeers',\n",
              "  'competing',\n",
              "  'for',\n",
              "  'the',\n",
              "  'million',\n",
              "  'ansari',\n",
              "  'prize',\n",
              "  'contest',\n",
              "  'for',\n",
              "  'privately',\n",
              "  'funded',\n",
              "  'suborbital',\n",
              "  'space',\n",
              "  'flight',\n",
              "  'has',\n",
              "  'officially',\n",
              "  'announced',\n",
              "  'the',\n",
              "  'first',\n",
              "  'launch',\n",
              "  'date',\n",
              "  'for',\n",
              "  'its',\n",
              "  'manned',\n",
              "  'rocket'],\n",
              " ['ky',\n",
              "  'company',\n",
              "  'wins',\n",
              "  'grant',\n",
              "  'to',\n",
              "  'study',\n",
              "  'peptides',\n",
              "  'ap',\n",
              "  'ap',\n",
              "  'company',\n",
              "  'founded',\n",
              "  'by',\n",
              "  'chemistry',\n",
              "  'researcher',\n",
              "  'at',\n",
              "  'the',\n",
              "  'university',\n",
              "  'of',\n",
              "  'louisville',\n",
              "  'won',\n",
              "  'grant',\n",
              "  'to',\n",
              "  'develop',\n",
              "  'method',\n",
              "  'of',\n",
              "  'producing',\n",
              "  'better',\n",
              "  'peptides',\n",
              "  'which',\n",
              "  'are',\n",
              "  'short',\n",
              "  'chains',\n",
              "  'of',\n",
              "  'amino',\n",
              "  'acids',\n",
              "  'the',\n",
              "  'building',\n",
              "  'blocks',\n",
              "  'of',\n",
              "  'proteins'],\n",
              " ['prediction',\n",
              "  'unit',\n",
              "  'helps',\n",
              "  'forecast',\n",
              "  'wildfires',\n",
              "  'ap',\n",
              "  'ap',\n",
              "  'it',\n",
              "  'barely',\n",
              "  'dawn',\n",
              "  'when',\n",
              "  'mike',\n",
              "  'fitzpatrick',\n",
              "  'starts',\n",
              "  'his',\n",
              "  'shift',\n",
              "  'with',\n",
              "  'blur',\n",
              "  'of',\n",
              "  'colorful',\n",
              "  'maps',\n",
              "  'figures',\n",
              "  'and',\n",
              "  'endless',\n",
              "  'charts',\n",
              "  'but',\n",
              "  'already',\n",
              "  'he',\n",
              "  'knows',\n",
              "  'what',\n",
              "  'the',\n",
              "  'day',\n",
              "  'will',\n",
              "  'bring',\n",
              "  'lightning',\n",
              "  'will',\n",
              "  'strike',\n",
              "  'in',\n",
              "  'places',\n",
              "  'he',\n",
              "  'expects',\n",
              "  'winds',\n",
              "  'will',\n",
              "  'pick',\n",
              "  'up',\n",
              "  'moist',\n",
              "  'places',\n",
              "  'will',\n",
              "  'dry',\n",
              "  'and',\n",
              "  'flames',\n",
              "  'will',\n",
              "  'roar'],\n",
              " ['calif',\n",
              "  'aims',\n",
              "  'to',\n",
              "  'limit',\n",
              "  'farm',\n",
              "  'related',\n",
              "  'smog',\n",
              "  'ap',\n",
              "  'ap',\n",
              "  'southern',\n",
              "  'california',\n",
              "  'smog',\n",
              "  'fighting',\n",
              "  'agency',\n",
              "  'went',\n",
              "  'after',\n",
              "  'emissions',\n",
              "  'of',\n",
              "  'the',\n",
              "  'bovine',\n",
              "  'variety',\n",
              "  'friday',\n",
              "  'adopting',\n",
              "  'the',\n",
              "  'nation',\n",
              "  'first',\n",
              "  'rules',\n",
              "  'to',\n",
              "  'reduce',\n",
              "  'air',\n",
              "  'pollution',\n",
              "  'from',\n",
              "  'dairy',\n",
              "  'cow',\n",
              "  'manure'],\n",
              " ['open',\n",
              "  'letter',\n",
              "  'against',\n",
              "  'british',\n",
              "  'copyright',\n",
              "  'indoctrination',\n",
              "  'in',\n",
              "  'schools',\n",
              "  'the',\n",
              "  'british',\n",
              "  'department',\n",
              "  'for',\n",
              "  'education',\n",
              "  'and',\n",
              "  'skills',\n",
              "  'dfes',\n",
              "  'recently',\n",
              "  'launched',\n",
              "  'music',\n",
              "  'manifesto',\n",
              "  'campaign',\n",
              "  'with',\n",
              "  'the',\n",
              "  'ostensible',\n",
              "  'intention',\n",
              "  'of',\n",
              "  'educating',\n",
              "  'the',\n",
              "  'next',\n",
              "  'generation',\n",
              "  'of',\n",
              "  'british',\n",
              "  'musicians',\n",
              "  'unfortunately',\n",
              "  'they',\n",
              "  'also',\n",
              "  'teamed',\n",
              "  'up',\n",
              "  'with',\n",
              "  'the',\n",
              "  'music',\n",
              "  'industry',\n",
              "  'emi',\n",
              "  'and',\n",
              "  'various',\n",
              "  'artists',\n",
              "  'to',\n",
              "  'make',\n",
              "  'this',\n",
              "  'popular',\n",
              "  'emi',\n",
              "  'has',\n",
              "  'apparently',\n",
              "  'negotiated',\n",
              "  'their',\n",
              "  'end',\n",
              "  'well',\n",
              "  'so',\n",
              "  'that',\n",
              "  'children',\n",
              "  'in',\n",
              "  'our',\n",
              "  'schools',\n",
              "  'will',\n",
              "  'now',\n",
              "  'be',\n",
              "  'indoctrinated',\n",
              "  'about',\n",
              "  'the',\n",
              "  'illegality',\n",
              "  'of',\n",
              "  'downloading',\n",
              "  'music',\n",
              "  'the',\n",
              "  'ignorance',\n",
              "  'and',\n",
              "  'audacity',\n",
              "  'of',\n",
              "  'this',\n",
              "  'got',\n",
              "  'to',\n",
              "  'me',\n",
              "  'little',\n",
              "  'so',\n",
              "  'wrote',\n",
              "  'an',\n",
              "  'open',\n",
              "  'letter',\n",
              "  'to',\n",
              "  'the',\n",
              "  'dfes',\n",
              "  'about',\n",
              "  'it',\n",
              "  'unfortunately',\n",
              "  'it',\n",
              "  'pedantic',\n",
              "  'as',\n",
              "  'suppose',\n",
              "  'you',\n",
              "  'have',\n",
              "  'to',\n",
              "  'be',\n",
              "  'when',\n",
              "  'writing',\n",
              "  'to',\n",
              "  'goverment',\n",
              "  'representatives',\n",
              "  'but',\n",
              "  'hope',\n",
              "  'you',\n",
              "  'find',\n",
              "  'it',\n",
              "  'useful',\n",
              "  'and',\n",
              "  'perhaps',\n",
              "  'feel',\n",
              "  'inspired',\n",
              "  'to',\n",
              "  'do',\n",
              "  'something',\n",
              "  'similar',\n",
              "  'if',\n",
              "  'or',\n",
              "  'when',\n",
              "  'the',\n",
              "  'same',\n",
              "  'thing',\n",
              "  'has',\n",
              "  'happened',\n",
              "  'in',\n",
              "  'your',\n",
              "  'area'],\n",
              " ['loosing',\n",
              "  'the',\n",
              "  'war',\n",
              "  'on',\n",
              "  'terrorism',\n",
              "  'sven',\n",
              "  'jaschan',\n",
              "  'self',\n",
              "  'confessed',\n",
              "  'author',\n",
              "  'of',\n",
              "  'the',\n",
              "  'netsky',\n",
              "  'and',\n",
              "  'sasser',\n",
              "  'viruses',\n",
              "  'is',\n",
              "  'responsible',\n",
              "  'for',\n",
              "  'percent',\n",
              "  'of',\n",
              "  'virus',\n",
              "  'infections',\n",
              "  'in',\n",
              "  'according',\n",
              "  'to',\n",
              "  'six',\n",
              "  'month',\n",
              "  'virus',\n",
              "  'roundup',\n",
              "  'published',\n",
              "  'wednesday',\n",
              "  'by',\n",
              "  'antivirus',\n",
              "  'company',\n",
              "  'sophos',\n",
              "  'the',\n",
              "  'year',\n",
              "  'old',\n",
              "  'jaschan',\n",
              "  'was',\n",
              "  'taken',\n",
              "  'into',\n",
              "  'custody',\n",
              "  'in',\n",
              "  'germany',\n",
              "  'in',\n",
              "  'may',\n",
              "  'by',\n",
              "  'police',\n",
              "  'who',\n",
              "  'said',\n",
              "  'he',\n",
              "  'had',\n",
              "  'admitted',\n",
              "  'programming',\n",
              "  'both',\n",
              "  'the',\n",
              "  'netsky',\n",
              "  'and',\n",
              "  'sasser',\n",
              "  'worms',\n",
              "  'something',\n",
              "  'experts',\n",
              "  'at',\n",
              "  'microsoft',\n",
              "  'confirmed',\n",
              "  'microsoft',\n",
              "  'antivirus',\n",
              "  'reward',\n",
              "  'program',\n",
              "  'led',\n",
              "  'to',\n",
              "  'the',\n",
              "  'teenager',\n",
              "  'arrest',\n",
              "  'during',\n",
              "  'the',\n",
              "  'five',\n",
              "  'months',\n",
              "  'preceding',\n",
              "  'jaschan',\n",
              "  'capture',\n",
              "  'there',\n",
              "  'were',\n",
              "  'at',\n",
              "  'least',\n",
              "  'variants',\n",
              "  'of',\n",
              "  'netsky',\n",
              "  'and',\n",
              "  'one',\n",
              "  'of',\n",
              "  'the',\n",
              "  'port',\n",
              "  'scanning',\n",
              "  'network',\n",
              "  'worm',\n",
              "  'sasser',\n",
              "  'graham',\n",
              "  'cluley',\n",
              "  'senior',\n",
              "  'technology',\n",
              "  'consultant',\n",
              "  'at',\n",
              "  'sophos',\n",
              "  'said',\n",
              "  'it',\n",
              "  'was',\n",
              "  'staggeri'],\n",
              " ['foafkey',\n",
              "  'foaf',\n",
              "  'pgp',\n",
              "  'key',\n",
              "  'distribution',\n",
              "  'and',\n",
              "  'bloom',\n",
              "  'filters',\n",
              "  'foaf',\n",
              "  'loaf',\n",
              "  'and',\n",
              "  'bloom',\n",
              "  'filters',\n",
              "  'have',\n",
              "  'lot',\n",
              "  'of',\n",
              "  'interesting',\n",
              "  'properties',\n",
              "  'for',\n",
              "  'social',\n",
              "  'network',\n",
              "  'and',\n",
              "  'whitelist',\n",
              "  'distribution',\n",
              "  'think',\n",
              "  'we',\n",
              "  'can',\n",
              "  'go',\n",
              "  'one',\n",
              "  'level',\n",
              "  'higher',\n",
              "  'though',\n",
              "  'and',\n",
              "  'include',\n",
              "  'gpg',\n",
              "  'openpgp',\n",
              "  'key',\n",
              "  'fingerpring',\n",
              "  'distribution',\n",
              "  'in',\n",
              "  'the',\n",
              "  'foaf',\n",
              "  'file',\n",
              "  'for',\n",
              "  'simple',\n",
              "  'web',\n",
              "  'of',\n",
              "  'trust',\n",
              "  'based',\n",
              "  'key',\n",
              "  'distribution',\n",
              "  'what',\n",
              "  'if',\n",
              "  'we',\n",
              "  'used',\n",
              "  'foaf',\n",
              "  'and',\n",
              "  'included',\n",
              "  'the',\n",
              "  'pgp',\n",
              "  'key',\n",
              "  'fingerprint',\n",
              "  'for',\n",
              "  'identities',\n",
              "  'this',\n",
              "  'could',\n",
              "  'mean',\n",
              "  'lot',\n",
              "  'you',\n",
              "  'include',\n",
              "  'the',\n",
              "  'pgp',\n",
              "  'key',\n",
              "  'fingerprints',\n",
              "  'within',\n",
              "  'the',\n",
              "  'foaf',\n",
              "  'file',\n",
              "  'of',\n",
              "  'your',\n",
              "  'direct',\n",
              "  'friends',\n",
              "  'and',\n",
              "  'then',\n",
              "  'include',\n",
              "  'bloom',\n",
              "  'filter',\n",
              "  'of',\n",
              "  'the',\n",
              "  'pgp',\n",
              "  'key',\n",
              "  'fingerprints',\n",
              "  'of',\n",
              "  'your',\n",
              "  'entire',\n",
              "  'whitelist',\n",
              "  'the',\n",
              "  'source',\n",
              "  'foaf',\n",
              "  'file',\n",
              "  'would',\n",
              "  'of',\n",
              "  'course',\n",
              "  'need',\n",
              "  'to',\n",
              "  'be',\n",
              "  'encrypted',\n",
              "  'your',\n",
              "  'whitelist',\n",
              "  'would',\n",
              "  'be',\n",
              "  'populated',\n",
              "  'from',\n",
              "  'the',\n",
              "  'social',\n",
              "  'network',\n",
              "  'as',\n",
              "  'your',\n",
              "  'client',\n",
              "  'discovered',\n",
              "  'new',\n",
              "  'identit'],\n",
              " ['mail',\n",
              "  'scam',\n",
              "  'targets',\n",
              "  'police',\n",
              "  'chief',\n",
              "  'wiltshire',\n",
              "  'police',\n",
              "  'warns',\n",
              "  'about',\n",
              "  'phishing',\n",
              "  'after',\n",
              "  'its',\n",
              "  'fraud',\n",
              "  'squad',\n",
              "  'chief',\n",
              "  'was',\n",
              "  'targeted'],\n",
              " ['card',\n",
              "  'fraud',\n",
              "  'unit',\n",
              "  'nets',\n",
              "  'cards',\n",
              "  'in',\n",
              "  'its',\n",
              "  'first',\n",
              "  'two',\n",
              "  'years',\n",
              "  'the',\n",
              "  'uk',\n",
              "  'dedicated',\n",
              "  'card',\n",
              "  'fraud',\n",
              "  'unit',\n",
              "  'has',\n",
              "  'recovered',\n",
              "  'stolen',\n",
              "  'cards',\n",
              "  'and',\n",
              "  'arrests',\n",
              "  'and',\n",
              "  'estimates',\n",
              "  'it',\n",
              "  'saved']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHhU41GWxg2X"
      },
      "source": [
        "## Create document embeddings\n",
        "Our first step is to build the vocabulary.  Essentially, the vocabulary is a list (accessible via `model.wv.index_to_key`) of all of the unique words extracted from the training corpus. Additional attributes for each word are available using the `model.wv.get_vecattr()` method.\n",
        "\n",
        "After our vocabulary is build, we train our embedding model using the training corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AVdbnX8wxg2X",
        "outputId": "f2d4414f-06ab-46c7-d436-f9205e154bc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"
          ]
        }
      ],
      "source": [
        "# Build vocabulary and train embedding model on the training corpus\n",
        "# dbow_words=0 uses pre-trained word embeddings and only trains the document embeddings\n",
        "doc2vec_model = gensim.models.doc2vec.Doc2Vec(vector_size=25, min_count=2, dbow_words=0, epochs=20)\n",
        "doc2vec_model.build_vocab(corpus)\n",
        "doc2vec_model.train(corpus, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAScXnYVxg2X"
      },
      "source": [
        "Now that our embedding model is trained, we can use it to get the embeddings for the training and test sets using `model.infer_vector()` for each document in the set.  We can then use the vectors as representations of the documents to do things such as evaluate similarity using cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n8QTOmgyxg2Y"
      },
      "outputs": [],
      "source": [
        "# Use the embedding model to get the embedding vectors for the training and test sets\n",
        "X_train = [doc2vec_model.infer_vector(doc) for doc in train_tokens]\n",
        "X_test = [doc2vec_model.infer_vector(doc) for doc in test_tokens]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[0]"
      ],
      "metadata": {
        "id": "CJEQb1qa1dQ0",
        "outputId": "1ff7232f-45cf-4a9c-e524-01004e718532",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.4254839 , -0.18550114, -0.01266643,  0.23160307, -0.13381748,\n",
              "       -0.18228108,  0.36365193,  0.25047034, -0.06107698,  0.22020012,\n",
              "       -0.28994694,  0.10834277, -0.12223433,  0.03980525, -0.27706906,\n",
              "        0.16899094, -0.15330431, -0.4889337 , -0.36030567, -0.2705503 ,\n",
              "        0.7236407 ,  0.150491  , -0.51067156,  0.07982385,  0.3432706 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmxeMThexg2Y"
      },
      "source": [
        "## Train classification model\n",
        "Finally, we will used our embeddings as features to train a softmax regression model to classify the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Trh5Snb2xg2Y",
        "outputId": "44650136-7070-4d76-f7fd-ab5866c82028",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the training set is 0.763\n"
          ]
        }
      ],
      "source": [
        "# Train a classification model using logistic regression classifier\n",
        "y_train = train_df['Class Index']\n",
        "logreg_model = LogisticRegression(solver='saga')\n",
        "logreg_model.fit(X_train,y_train)\n",
        "preds = logreg_model.predict(X_train)\n",
        "acc = sum(preds==y_train)/len(y_train)\n",
        "print('Accuracy on the training set is {:.3f}'.format(acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqR_k9GExg2Z"
      },
      "source": [
        "## Evaluate model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IC5BYooixg2Z",
        "outputId": "6f8248d0-fbf2-43d9-dd9c-7cf535911223",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the training set is 0.757\n"
          ]
        }
      ],
      "source": [
        "# Evaluate performance on the test set\n",
        "y_test = test_df['Class Index']\n",
        "preds = logreg_model.predict(X_test)\n",
        "acc = sum(preds==y_test)/len(y_test)\n",
        "print('Accuracy on the training set is {:.3f}'.format(acc))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qo3ExALH1t9I"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JNPP4rsPc6Ag"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}